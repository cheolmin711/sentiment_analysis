{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\yok018\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     C:\\Users\\yok018\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\yok018\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\yok018\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# All modules that are required to import:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# For purpose of calculating execution time:\n",
    "start = time.time()\n",
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "pos_tweet = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tweet = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the stop words\n",
    "# borrowed list of stop words from https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt \n",
    "\n",
    "filepath = open(\"terrier-stop.txt\", \"r\")\n",
    "temp = filepath.read().split(\"\\n\")\n",
    "stop_words = { key : 1 for key in temp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of this part: Read through all positive / negative tweets, normalize and remove unnecessary words from tweets, then create actual dictionary-like to use for our dataset\n",
    "\n",
    "# Convert all complex part-of-speech to basic words\n",
    "# List of part-of-speech is in this link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "# WordNetLemmatizer has a function lemmatize where you can convert complex part of speech words into basic forms\n",
    "# Things to consider:\n",
    "#   Remove all unnecessary words from normalized_neg_tweets / normalized_pos_tweets\n",
    "#   1. Remove mentions(starts with @)\n",
    "#   2. Remove links (starts with https:// or http:// )\n",
    "#   3. Remove punctuation (starts with ! or ?)\n",
    "#   4. Remove Stop-Words (words that do have little to no meaning and does not affect the context of the sentence) to make our dataset more concise\n",
    "# Note that we are keeping emoji (i.e. :) or :( . That is because these emojis do actually show sentiment of the text context)\n",
    "# If words are DETERMINERS (DT), COORDINATING CONJUCTIONS (CC), PREPOSITIONS (IN), PERSONAL / POSSESSIVE PRONOUNS (PRP / PRP$), or WH-PRONOUNS (WP) WH-ADVERB(WRB), we remove it (consider as Stop words)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "punctuation_and_stop_words = {'!': 1, '\"': 1, '#': 1, '$': 1 ,'%': 1, '&': 1, \"'\": 1,'(': 1,')': 1,'*': 1,'+': 1,',': 1,'-': 1,'.': 1,':': 1,';': 1,'<': 1,'=': 1,'>': 1,'?': 1,'@': 1,'[': 1,']': 1,'^': 1,'_': 1,'`': 1,'{': 1,'|': 1,'}': 1,'~': 1,'https://': 1,'http://': 1}\n",
    "stop_words_final = {**stop_words, **punctuation_and_stop_words}\n",
    "\n",
    "\n",
    "def determiners(word):\n",
    "    if word in stop_words_final:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def normalize(tweet_list):\n",
    "    normalized_tweet = []\n",
    "    for tweet in tweet_list:\n",
    "        sentence = []\n",
    "        for token, tag in pos_tag(tweet):\n",
    "            # For Complex Noun words:\n",
    "            if tag.startswith('NN'):\n",
    "                new_tag = 'n'\n",
    "            # For Complex Verb words\n",
    "            elif tag.startswith('VB'):\n",
    "                new_tag = 'v'\n",
    "            # For stop-words\n",
    "            elif tag.startswith('DT') or tag.startswith('CC') or tag.startswith('IN') or tag.startswith('PRP') or tag.startswith('PRP$') or tag.startswith('WP') or tag.startswith('WRB'):\n",
    "                continue \n",
    "            # Every other words, convert them into adjective (pos = 'a')\n",
    "            else:\n",
    "                if determiners(token):\n",
    "                    new_tag = 'a'\n",
    "                else:\n",
    "                    continue\n",
    "            sentence.append(normalizer.lemmatize(token, new_tag))\n",
    "        normalized_tweet.append(sentence)\n",
    "    return normalized_tweet\n",
    "\n",
    "normalized_pos_tweets = normalize(pos_tweet)\n",
    "normalized_neg_tweets = normalize(neg_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, store all positive / negative words into dictionary so it can be used as a guide for calculating sentiment for sentences\n",
    "\n",
    "pos_words_dict = {}\n",
    "neg_words_dict = {}\n",
    "\n",
    "# Store all words into dictionary\n",
    "for tweet in normalized_pos_tweets:\n",
    "    for word in tweet:\n",
    "        if word in pos_words_dict:\n",
    "            temp = pos_words_dict[word.lower()]\n",
    "            temp += 1\n",
    "            pos_words_dict[word.lower()] = temp\n",
    "        else:\n",
    "            pos_words_dict[word.lower()] = 1\n",
    "\n",
    "for tweet in normalized_neg_tweets:\n",
    "    for word in tweet:\n",
    "        if word in neg_words_dict:\n",
    "            temp = neg_words_dict[word.lower()]\n",
    "            temp += 1\n",
    "            neg_words_dict[word.lower()] = temp\n",
    "        else:\n",
    "            neg_words_dict[word.lower()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all emojis and leave only roman alphabets\n",
    "pos_df = pd.DataFrame({'word': list(pos_words_dict.keys()), 'frequency': list(pos_words_dict.values())})\n",
    "cleaned_pos_df = pos_df.loc[pos_df['word'].str.isalpha()]\n",
    "\n",
    "neg_df = pd.DataFrame({'word': list(neg_words_dict.keys()), 'frequency': list(neg_words_dict.values())})\n",
    "cleaned_neg_df = neg_df.loc[neg_df['word'].str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency\n",
       "0             be         97.0         57.0       40.0\n",
       "1            top         10.0          6.0        4.0\n",
       "2         engage          7.0          0.0        7.0\n",
       "3         member         16.0          6.0       10.0\n",
       "4      community          2.0          1.0        1.0\n",
       "...          ...          ...          ...        ...\n",
       "9446       ahmad          0.0          1.0       -1.0\n",
       "9447      maslan          0.0          1.0       -1.0\n",
       "9448        hull          0.0          1.0       -1.0\n",
       "9449   supporter          0.0          1.0       -1.0\n",
       "9450  misserable          0.0          1.0       -1.0\n",
       "\n",
       "[9451 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9446</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9447</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9448</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9449</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9450</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9451 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# merge the two dataframes into one\n",
    "merged = pd.merge(cleaned_pos_df, cleaned_neg_df, on='word', how='outer').fillna(0)\n",
    "merged['frequency'] = merged['frequency_x'] - merged['frequency_y']\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency     scale\n",
       "0             be         97.0         57.0       40.0  3.133333\n",
       "1            top         10.0          6.0        4.0  1.213333\n",
       "2         engage          7.0          0.0        7.0  1.373333\n",
       "3         member         16.0          6.0       10.0  1.533333\n",
       "4      community          2.0          1.0        1.0  1.053333\n",
       "...          ...          ...          ...        ...       ...\n",
       "9446       ahmad          0.0          1.0       -1.0 -1.012270\n",
       "9447      maslan          0.0          1.0       -1.0 -1.012270\n",
       "9448        hull          0.0          1.0       -1.0 -1.012270\n",
       "9449   supporter          0.0          1.0       -1.0 -1.012270\n",
       "9450  misserable          0.0          1.0       -1.0 -1.012270\n",
       "\n",
       "[9451 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n      <td>3.133333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>1.213333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.373333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>1.533333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.053333</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9446</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9447</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9448</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9449</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9450</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n  </tbody>\n</table>\n<p>9451 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# scale the frequencies of each word between 1 to 5 for positive words, -1 to -5 for negative words\n",
    "# if there is a duplicate word in both negative and positive dataset, take the difference in frequencies\n",
    "# and consider it as a positive word if the positive frequency is higher, and vice versa\n",
    "\n",
    "pos_max = merged['frequency'].max()\n",
    "neg_min = abs(merged['frequency'].min())\n",
    "\n",
    "def scaler(freq):\n",
    "\n",
    "    if freq > 0:\n",
    "        return freq * (4 / pos_max) + 1\n",
    "    elif freq < 0:\n",
    "        return freq * (4 / neg_min) - 1\n",
    "merged = merged.assign(**{'scale':merged['frequency'].apply(scaler)})\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         word  frequency_x  frequency_y  frequency  scale  is_null\n",
       "6         hey          1.0          1.0        0.0    NaN     True\n",
       "7       james          1.0          1.0        0.0    NaN     True\n",
       "8         how          1.0          1.0        0.0    NaN     True\n",
       "13     centre          2.0          2.0        0.0    NaN     True\n",
       "17       many          1.0          1.0        0.0    NaN     True\n",
       "...       ...          ...          ...        ...    ...      ...\n",
       "5884    lagos          1.0          1.0        0.0    NaN     True\n",
       "5886  kingdom          1.0          1.0        0.0    NaN     True\n",
       "5887   potato          1.0          1.0        0.0    NaN     True\n",
       "5888  hundred          1.0          1.0        0.0    NaN     True\n",
       "5899  distant          1.0          1.0        0.0    NaN     True\n",
       "\n",
       "[981 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n      <th>is_null</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>hey</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>james</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>how</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>centre</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>many</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5884</th>\n      <td>lagos</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5886</th>\n      <td>kingdom</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5887</th>\n      <td>potato</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5888</th>\n      <td>hundred</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5899</th>\n      <td>distant</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>981 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "merged = merged.assign(**{'is_null': merged['scale'].isnull().values})\n",
    "merged.loc[merged['is_null'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency     scale  is_null\n",
       "0             be         97.0         57.0       40.0  3.133333    False\n",
       "1            top         10.0          6.0        4.0  1.213333    False\n",
       "2         engage          7.0          0.0        7.0  1.373333    False\n",
       "3         member         16.0          6.0       10.0  1.533333    False\n",
       "4      community          2.0          1.0        1.0  1.053333    False\n",
       "...          ...          ...          ...        ...       ...      ...\n",
       "9446       ahmad          0.0          1.0       -1.0 -1.012270    False\n",
       "9447      maslan          0.0          1.0       -1.0 -1.012270    False\n",
       "9448        hull          0.0          1.0       -1.0 -1.012270    False\n",
       "9449   supporter          0.0          1.0       -1.0 -1.012270    False\n",
       "9450  misserable          0.0          1.0       -1.0 -1.012270    False\n",
       "\n",
       "[8470 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n      <th>is_null</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n      <td>3.133333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>1.213333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.373333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>1.533333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.053333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9446</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9447</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9448</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9449</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9450</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>8470 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# decided to drop words with a total frequency of zero, since they were words that appeared the same number of times as both negative and positive words\n",
    "merged = merged.dropna()\n",
    "# Finished creating dataset for pos / neg words with scores (scale) included\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.nytimes.com/2020/12/16/technology/facebook-takes-the-gloves-off-in-feud-with-apple.html',\n",
       " 'https://www.nytimes.com/2020/12/13/business/media/apple-gawker-tim-cook.html',\n",
       " 'https://www.nytimes.com/2020/12/23/business/dealbook/trump-stimulus-veto.html',\n",
       " 'https://www.nytimes.com/2020/12/01/technology/amazon-apple-chips-intel-arm.html',\n",
       " 'https://www.nytimes.com/2020/12/17/technology/google-antitrust-monopoly.html',\n",
       " 'https://www.nytimes.com/2020/12/17/business/dealbook/tech-apple-facebook-fight.html',\n",
       " 'https://www.nytimes.com/2020/12/15/technology/big-tech-regulation-europe.html',\n",
       " 'https://www.nytimes.com/2020/12/14/technology/big-tech-lobbying-europe.html',\n",
       " 'https://www.nytimes.com/2020/12/09/technology/personaltech/amazon-halo-review.html',\n",
       " 'https://www.nytimes.com/2020/11/18/technology/apple-app-store-fee.html']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Now, retrieve articles from NYT (using API provided from NYT)\n",
    "response = requests.get(\"https://api.nytimes.com/svc/search/v2/articlesearch.json?q=apple&fq=news_desk:Business&api-key=fO0tDSRQQdU68GkuXbMjt1uA2FYImzVp\").json()\n",
    "docs = response['response']['docs']\n",
    "url_list = []\n",
    "for item in docs:\n",
    "    url_list.append(item['web_url'])\n",
    "article_list = []\n",
    "url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "title = []\n",
    "abstract = []\n",
    "for url in url_list:\n",
    "    time.sleep(0.1)\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36'}\n",
    "    article = requests.get(url, headers = headers)\n",
    "    soup = bs4.BeautifulSoup(article.content, 'html.parser')\n",
    "    article_text_p = soup.find_all('p', attrs={'class': 'css-axufdj evys1bk0'})\n",
    "    abstract_text_p = soup.find('p', attrs={'class': 'css-w6ymp8 e1wiw3jv0'})\n",
    "    title_text_h1 = soup.find('h1', attrs={'data-test-id': 'headline'})\n",
    "    temp = []\n",
    "    title.append(title_text_h1.text)\n",
    "    abstract.append(abstract_text_p.text)\n",
    "    \n",
    "    for item in article_text_p:\n",
    "        temp.append(item.text)\n",
    "    space = ' '\n",
    "    article_text = space.join(temp)\n",
    "    text.append(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization to sentences \n",
    "\n",
    "tokenized_by_sentence = []\n",
    "for num in range(len(text)):\n",
    "    del_quo = re.sub(\",”\", \" \", text[num])\n",
    "    del_quo_2  = re.sub(\"”\", \" \", del_quo)\n",
    "    del_quo_3 = re.sub(\"“\", \"\", del_quo_2)\n",
    "    text_token = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|;|”)\\s\", del_quo_3)\n",
    "    text_token.insert(0, abstract[num])\n",
    "    text_token.insert(0, title[num])\n",
    "    tokenized_by_sentence.append(text_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and lametize the article from new york times\n",
    "\n",
    "def stop_word_filter(word):\n",
    "    if (word in stop_words): \n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "\n",
    "# For filtering out empty strings\n",
    "stop_words[''] = 1\n",
    "\n",
    "def tokenizer_myself(given_articles):\n",
    "    tokenized_result = []\n",
    "    for article_iter in given_articles: \n",
    "        temp = []\n",
    "        for sentence in article_iter:\n",
    "            tokenized_sentence = sentence.split(\" \")\n",
    "            tokenized_sentence = list(filter(stop_word_filter, tokenized_sentence))\n",
    "            if len(tokenized_sentence) > 1:\n",
    "                temp.append(tokenized_sentence)\n",
    "        new_temp = normalize(temp)\n",
    "        tokenized_result.append(new_temp)\n",
    "    return tokenized_result\n",
    "\n",
    "tokenized_by_sentence_new = tokenizer_myself(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate positivity or negativity of each sentence\n",
    "\n",
    "articles_lst = []\n",
    "\n",
    "hash_table = { key:1 for key in list(merged['word'])}\n",
    "\n",
    "for article in tokenized_by_sentence_new:\n",
    "    sentence_vals = []\n",
    "    for sentence in article:\n",
    "        val = 1.0\n",
    "        for word in sentence:\n",
    "            if word in hash_table:\n",
    "                val = val * merged.loc[merged['word'] == word]['scale'].values[0]\n",
    "        sentence_vals.append(val)\n",
    "    articles_lst.append(sentence_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.776708063515334,\n",
       " -0.19211078597161274,\n",
       " -3.1607233883820003,\n",
       " 1.9378043608143205,\n",
       " 0.05862496784129545,\n",
       " -0.10573732801823434,\n",
       " 2.5652842585009497,\n",
       " 1.983412941237357,\n",
       " -1.2840119590627617,\n",
       " -2.171577842704367]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Calculate the overall percent for the article (50% for Title and subtitle, other 50% for content)\n",
    "# Note that we are removing score 1 since those scores mean that our system did not find any pos / neg words from that sentence \n",
    "\n",
    "def filter_one(variable):\n",
    "    one_ind = 1.0\n",
    "    if variable == one_ind:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "avg_score_article = []\n",
    "for article in articles_lst:\n",
    "    new_article = list(filter(filter_one, article))\n",
    "    avg_score = (sum(new_article[0:2]) / 2) + (sum(new_article[2:]) / len(new_article[2:])) / 2\n",
    "    avg_score_article.append(avg_score)\n",
    "\n",
    "avg_score_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total execution time:  16.69343590736389\n"
     ]
    }
   ],
   "source": [
    "# For purpose of calculating execution time\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total execution time: \", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}